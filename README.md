# Machine Unlearning for Single-Cell VAEs

STAT 4243 Final Project — Columbia University

## Overview

Eight machine unlearning methods are evaluated on VAEs trained on single-cell RNA sequencing data (PBMC-33k and Tabula Muris). All eight fail on structured (biologically coherent) forget sets. A Fisher information analysis explains why: the VAE's shared decoder creates 17x higher Fisher overlap between forget and retain sets than a classifier on the same data, making selective parameter perturbation structurally harder for generative models.

## Setup

```bash
# Create environment
pyenv activate stat4243  # or: pip install -r requirements.txt

# Unzip raw data
cd data/filtered_gene_bc_matrices && unzip hg19.zip && cd ../..

# Verify
python -c "import torch; print(f'PyTorch {torch.__version__}')"
```

## Project structure

```
src/
├── vae.py                          # VAE (NB likelihood, z=32, hidden=[1024,512,128])
├── vae_conditional.py              # Cluster-conditional VAE variant
├── attacker.py                     # MIA attacker (spectral norm, 69-dim features)
├── attacker_eval.py                # Post-hoc attacker evaluation
├── fisher_utils.py                 # Datasets, metrics, Fisher functions for NB26-30
├── train.py                        # Baseline VAE training
├── train_unlearn_extragradient.py  # Extra-gradient co-training
├── train_fisher_unlearn.py         # Fisher scrubbing
├── train_ssd.py                    # Selective Synaptic Dampening
├── train_scrub.py                  # SCRUB teacher-student
├── train_contrastive_unlearn.py    # Contrastive latent unlearning
├── train_retain_finetune.py        # Retain-only fine-tuning
├── train_gradient_ascent.py        # Gradient ascent + fine-tune
├── train_dp.py                     # DP-SGD via Opacus
├── retrain.py                      # Full retraining baseline
└── utility_metrics.py              # ELBO, silhouette, ARI, marker correlation

notebooks/                          # Experiments (01-30, run in order)
scripts/                            # Multi-seed pipelines
outputs/
├── p1/                             # Baseline models and data splits
├── p2/                             # Unlearning method results
├── p3/                             # MoG simulations
├── p4/                             # Ablations (multiseed, size, lambda)
└── p6/                             # Fisher overlap analysis (NB26-30)

data/
├── filtered_gene_bc_matrices/      # Raw 10x PBMC data (unzip hg19.zip)
├── adata_processed.h5ad            # Generated by notebook 01
└── tabula_muris_processed.h5ad     # Generated by notebook 01

Writeup.tex                         # Paper (LaTeX, NeurIPS format)
references.bib                      # Bibliography (25 references)
figures/                            # Generated figures
```

## Reproducing results

Run notebooks in numerical order:

```bash
jupyter notebook notebooks/
```

Notebooks 01-10 cover data prep, baseline training, and initial unlearning experiments.
Notebooks 11-25 cover additional methods, cross-dataset validation, ablations, and attack diversity.
Notebooks 26-30 cover the Fisher overlap analysis (Section 6 of the paper):

- **NB26**: Canonical Fisher overlap (VAE vs classifier, damping=1e-8)
- **NB27**: Deep MLP classifier (fair capacity comparison)
- **NB28**: VAE z=8 variant (architecture generalization)
- **NB29**: Conditional VAE (cluster-specific output columns)
- **NB30**: Proposition 1 verification + conditional VAE scrubbing

## Key results

All methods evaluated on PBMC-33k structured forget set (cluster 13, n=30 megakaryocytes). Advantage = 2|AUC - 0.5|. Target: advantage within retrain 95% CI (upper bound = 0.266).

| Method | Seeds | AUC | Advantage | Status |
|--------|-------|-----|-----------|--------|
| Baseline | — | 0.783 | 0.565 | — |
| Retain-only fine-tune | 5 | 0.665 | 0.331 | FAIL |
| Gradient ascent | 5 | 0.702 | 0.404 | FAIL |
| SSD (alpha=1.0) | 3 | 0.725 | 0.450 | FAIL |
| SCRUB (alpha_f=1.0) | 3 | 0.737 | 0.474 | FAIL |
| Contrastive latent | 3 | 0.153 | 0.695 | FAIL (Streisand) |
| Fisher scrubbing | 3 | 0.814 | 0.628 | FAIL (worse) |
| Extra-gradient (lambda=10) | 10 | 0.429 | 0.300 | FAIL |
| DP-SGD (eps=10) | 3 | 0.464 | 0.072 | Near target |
| **Retrain** | — | **0.523** | **0.046** | **TARGET** |

Fisher overlap (Section 6): VAE global cosine = 0.306, classifier = 0.018, ratio = 17x.

## Hardware

- Apple M-series laptop (CPU only) or CUDA GPU
- 16GB RAM minimum
- ~10GB disk

Typical times (CPU): baseline VAE ~20 min, extra-gradient ~80 min, Fisher ~2 min, DP-SGD ~250 min.

## Citation

Code: https://github.com/db-d2/Machine_Unlearning
