{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 22. Contrastive Latent Unlearning Experiments\n",
    "\n",
    "A VAE-specific unlearning approach: push forget-set latent representations\n",
    "toward the prior N(0, I) while preserving retain-set representations.\n",
    "If forget samples map to the prior, they become indistinguishable from\n",
    "random noise and carry no membership signal.\n",
    "\n",
    "Phase 1: Contrastive training (encoder only)\n",
    "Phase 2: Retain fine-tuning (full model)\n",
    "\n",
    "Tested on PBMC structured forget set with 3 seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "from train_contrastive_unlearn import train_contrastive\n",
    "\n",
    "DATA_PATH = '../data/adata_processed.h5ad'\n",
    "SPLIT_PATH = '../outputs/p1/split_structured.json'\n",
    "BASELINE_CKPT = '../outputs/p1/baseline/best_model.pt'\n",
    "OUTPUT_BASE = Path('../outputs/p2/contrastive')\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Train Contrastive Unlearning with 3 Seeds\n",
    "\n",
    "Hyperparameters:\n",
    "- gamma=1.0 (forget prior-matching weight)\n",
    "- lam=1.0 (retain preservation weight)\n",
    "- 20 contrastive epochs, encoder only, lr=1e-4\n",
    "- 10 retain fine-tuning epochs, full model, lr=1e-4\n",
    "\n",
    "Also sweep gamma in {0.1, 1.0, 10.0} with seed=42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Contrastive gamma=1.0, seed=42\n",
      "============================================================\n",
      "Data: torch.Size([33088, 2000]), Forget: 30, Retain: 28094, Device: cpu\n",
      "Phase 1: Contrastive training (20 epochs, gamma=1.0, lam=1.0)...\n",
      "  Epoch 1: forget_kl=1.7475, retain_dist=0.1564\n",
      "  Epoch 5: forget_kl=0.0772, retain_dist=0.0192\n",
      "  Epoch 10: forget_kl=0.0420, retain_dist=0.0155\n",
      "  Epoch 15: forget_kl=0.0241, retain_dist=0.0138\n",
      "  Epoch 20: forget_kl=0.0122, retain_dist=0.0129\n",
      "Phase 2: Retain fine-tuning (10 epochs, lr=0.0001)...\n",
      "  Epoch 1: train=366.31, val=358.18\n",
      "  Epoch 5: train=365.74, val=357.85\n",
      "  Epoch 10: train=365.55, val=357.46\n",
      "Saved to ../outputs/p2/contrastive/gamma1.0_seed42/best_model.pt\n",
      "Done in 161.2s\n",
      "\n",
      "============================================================\n",
      "Contrastive gamma=1.0, seed=123\n",
      "============================================================\n",
      "Data: torch.Size([33088, 2000]), Forget: 30, Retain: 28094, Device: cpu\n",
      "Phase 1: Contrastive training (20 epochs, gamma=1.0, lam=1.0)...\n",
      "  Epoch 1: forget_kl=1.7542, retain_dist=0.1563\n",
      "  Epoch 5: forget_kl=0.0784, retain_dist=0.0194\n",
      "  Epoch 10: forget_kl=0.0429, retain_dist=0.0156\n",
      "  Epoch 15: forget_kl=0.0249, retain_dist=0.0139\n",
      "  Epoch 20: forget_kl=0.0125, retain_dist=0.0129\n",
      "Phase 2: Retain fine-tuning (10 epochs, lr=0.0001)...\n",
      "  Epoch 1: train=366.28, val=361.79\n",
      "  Epoch 5: train=365.82, val=361.65\n",
      "  Epoch 10: train=365.39, val=361.13\n",
      "Saved to ../outputs/p2/contrastive/gamma1.0_seed123/best_model.pt\n",
      "Done in 167.4s\n",
      "\n",
      "============================================================\n",
      "Contrastive gamma=1.0, seed=456\n",
      "============================================================\n",
      "Data: torch.Size([33088, 2000]), Forget: 30, Retain: 28094, Device: cpu\n",
      "Phase 1: Contrastive training (20 epochs, gamma=1.0, lam=1.0)...\n",
      "  Epoch 1: forget_kl=1.7381, retain_dist=0.1552\n",
      "  Epoch 5: forget_kl=0.0785, retain_dist=0.0194\n",
      "  Epoch 10: forget_kl=0.0419, retain_dist=0.0155\n",
      "  Epoch 15: forget_kl=0.0247, retain_dist=0.0138\n",
      "  Epoch 20: forget_kl=0.0122, retain_dist=0.0129\n",
      "Phase 2: Retain fine-tuning (10 epochs, lr=0.0001)...\n",
      "  Epoch 1: train=366.27, val=362.45\n",
      "  Epoch 5: train=365.75, val=362.09\n",
      "  Epoch 10: train=365.36, val=362.20\n",
      "Saved to ../outputs/p2/contrastive/gamma1.0_seed456/best_model.pt\n",
      "Done in 168.1s\n",
      "\n",
      "============================================================\n",
      "Contrastive gamma=0.1, seed=42\n",
      "============================================================\n",
      "Data: torch.Size([33088, 2000]), Forget: 30, Retain: 28094, Device: cpu\n",
      "Phase 1: Contrastive training (20 epochs, gamma=0.1, lam=1.0)...\n",
      "  Epoch 1: forget_kl=0.1764, retain_dist=0.0552\n",
      "  Epoch 5: forget_kl=0.0081, retain_dist=0.0136\n",
      "  Epoch 10: forget_kl=0.0046, retain_dist=0.0117\n",
      "  Epoch 15: forget_kl=0.0028, retain_dist=0.0109\n",
      "  Epoch 20: forget_kl=0.0016, retain_dist=0.0105\n",
      "Phase 2: Retain fine-tuning (10 epochs, lr=0.0001)...\n",
      "  Epoch 1: train=366.14, val=358.05\n",
      "  Epoch 5: train=365.66, val=357.81\n",
      "  Epoch 10: train=365.51, val=357.41\n",
      "Saved to ../outputs/p2/contrastive/gamma0.1_seed42/best_model.pt\n",
      "Done in 175.1s\n",
      "\n",
      "============================================================\n",
      "Contrastive gamma=10.0, seed=42\n",
      "============================================================\n",
      "Data: torch.Size([33088, 2000]), Forget: 30, Retain: 28094, Device: cpu\n",
      "Phase 1: Contrastive training (20 epochs, gamma=10.0, lam=1.0)...\n",
      "  Epoch 1: forget_kl=17.4970, retain_dist=0.2289\n",
      "  Epoch 5: forget_kl=0.7625, retain_dist=0.0343\n",
      "  Epoch 10: forget_kl=0.4126, retain_dist=0.0237\n",
      "  Epoch 15: forget_kl=0.2413, retain_dist=0.0204\n",
      "  Epoch 20: forget_kl=0.1276, retain_dist=0.0187\n",
      "Phase 2: Retain fine-tuning (10 epochs, lr=0.0001)...\n",
      "  Epoch 1: train=366.69, val=358.34\n",
      "  Epoch 5: train=365.88, val=357.93\n",
      "  Epoch 10: train=365.64, val=357.50\n",
      "Saved to ../outputs/p2/contrastive/gamma10.0_seed42/best_model.pt\n",
      "Done in 167.5s\n",
      "\n",
      "All training complete. 5 checkpoints saved.\n"
     ]
    }
   ],
   "source": [
    "SEEDS = [42, 123, 456]\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Multi-seed with default gamma=1.0\n",
    "for seed in SEEDS:\n",
    "    out_dir = OUTPUT_BASE / f'gamma1.0_seed{seed}'\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Contrastive gamma=1.0, seed={seed}')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    t0 = time.time()\n",
    "    ckpt_path = train_contrastive(\n",
    "        baseline_checkpoint=BASELINE_CKPT,\n",
    "        data_path=DATA_PATH,\n",
    "        split_path=SPLIT_PATH,\n",
    "        output_dir=str(out_dir),\n",
    "        gamma=1.0,\n",
    "        lam=1.0,\n",
    "        n_epochs=20,\n",
    "        lr=1e-4,\n",
    "        finetune_epochs=10,\n",
    "        finetune_lr=1e-4,\n",
    "        patience=10,\n",
    "        batch_size=256,\n",
    "        seed=seed,\n",
    "    )\n",
    "    elapsed = time.time() - t0\n",
    "    results[f'gamma1.0_seed{seed}'] = {'path': str(ckpt_path), 'time': elapsed}\n",
    "    print(f'Done in {elapsed:.1f}s')\n",
    "\n",
    "# Gamma sweep with seed=42\n",
    "for gamma in [0.1, 10.0]:\n",
    "    out_dir = OUTPUT_BASE / f'gamma{gamma}_seed42'\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Contrastive gamma={gamma}, seed=42')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    t0 = time.time()\n",
    "    ckpt_path = train_contrastive(\n",
    "        baseline_checkpoint=BASELINE_CKPT,\n",
    "        data_path=DATA_PATH,\n",
    "        split_path=SPLIT_PATH,\n",
    "        output_dir=str(out_dir),\n",
    "        gamma=gamma,\n",
    "        lam=1.0,\n",
    "        n_epochs=20,\n",
    "        lr=1e-4,\n",
    "        finetune_epochs=10,\n",
    "        finetune_lr=1e-4,\n",
    "        patience=10,\n",
    "        batch_size=256,\n",
    "        seed=42,\n",
    "    )\n",
    "    elapsed = time.time() - t0\n",
    "    results[f'gamma{gamma}_seed42'] = {'path': str(ckpt_path), 'time': elapsed}\n",
    "    print(f'Done in {elapsed:.1f}s')\n",
    "\n",
    "print(f'\\nAll training complete. {len(results)} checkpoints saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Evaluate with Canonical Fresh Attacker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fresh attacker on baseline F vs matched:\n",
      "  Samples: 224 (30 forget + 194 matched)\n",
      "  Features: 70 dims\n",
      "  Train: 179, Test: 45\n",
      "  Baseline AUC (F vs matched, full set): 0.7792\n",
      "  (Canonical NB03 value: ~0.769)\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, '../scripts')\n",
    "from eval_multiseed import (\n",
    "    load_vae_model, train_fresh_attacker, evaluate_privacy,\n",
    "    evaluate_utility, MARKER_GENES\n",
    ")\n",
    "import scanpy as sc\n",
    "\n",
    "adata = sc.read_h5ad(DATA_PATH)\n",
    "X = torch.tensor(\n",
    "    adata.X.toarray() if hasattr(adata.X, 'toarray') else adata.X,\n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "with open(SPLIT_PATH) as f:\n",
    "    split = json.load(f)\n",
    "forget_idx = split['forget_indices']\n",
    "retain_idx = split['retain_indices']\n",
    "unseen_idx = split['unseen_indices']\n",
    "\n",
    "with open('../outputs/p1.5/s1_matched_negatives.json') as f:\n",
    "    matched_data = json.load(f)\n",
    "matched_neg_idx = matched_data['matched_indices']\n",
    "\n",
    "X_holdout = X[unseen_idx]\n",
    "labels_holdout = adata.obs['leiden'].values[unseen_idx]\n",
    "gene_names = list(adata.var_names)\n",
    "marker_idx = [gene_names.index(g) for g in MARKER_GENES if g in gene_names]\n",
    "marker_names = [g for g in MARKER_GENES if g in gene_names]\n",
    "\n",
    "baseline_model, _ = load_vae_model(BASELINE_CKPT)\n",
    "attacker = train_fresh_attacker(\n",
    "    baseline_model, adata, forget_idx, matched_neg_idx, retain_idx, seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating gamma1.0_seed42...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dbenson/.pyenv/versions/stat4243/lib/python3.12/site-packages/threadpoolctl.py:1226: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  AUC=0.166, Advantage=0.667, ELBO=364.1, Marker r=0.832\n",
      "\n",
      "Evaluating gamma1.0_seed123...\n",
      "  AUC=0.182, Advantage=0.635, ELBO=363.9, Marker r=0.831\n",
      "\n",
      "Evaluating gamma1.0_seed456...\n",
      "  AUC=0.109, Advantage=0.782, ELBO=364.2, Marker r=0.831\n",
      "\n",
      "Evaluating gamma0.1_seed42...\n",
      "  AUC=0.107, Advantage=0.785, ELBO=364.0, Marker r=0.831\n",
      "\n",
      "Evaluating gamma10.0_seed42...\n",
      "  AUC=0.267, Advantage=0.465, ELBO=364.0, Marker r=0.832\n"
     ]
    }
   ],
   "source": [
    "eval_results = {}\n",
    "\n",
    "for name, info in results.items():\n",
    "    ckpt_path = info['path']\n",
    "    print(f'\\nEvaluating {name}...')\n",
    "    \n",
    "    model, config = load_vae_model(ckpt_path)\n",
    "    \n",
    "    privacy = evaluate_privacy(\n",
    "        model, attacker, adata, forget_idx, matched_neg_idx, retain_idx\n",
    "    )\n",
    "    utility = evaluate_utility(\n",
    "        model, X_holdout, labels_holdout, marker_idx, gene_names\n",
    "    )\n",
    "    \n",
    "    eval_results[name] = {\n",
    "        'privacy': privacy,\n",
    "        'utility': utility,\n",
    "        'training_time': info['time'],\n",
    "    }\n",
    "    \n",
    "    print(f'  AUC={privacy[\"mlp_auc\"]:.3f}, '\n",
    "          f'Advantage={privacy[\"mlp_advantage\"]:.3f}, '\n",
    "          f'ELBO={utility[\"elbo\"]:.1f}, '\n",
    "          f'Marker r={utility[\"marker_r\"]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contrastive Results (gamma=1.0, 3 seeds):\n",
      "  AUC: 0.153 +/- 0.032\n",
      "  Advantage: 0.695 +/- 0.063\n",
      "\n",
      "Gamma sweep (seed=42):\n",
      "Gamma           AUC  Advantage   Marker r     ELBO\n",
      "--------------------------------------------------\n",
      "0.1           0.107      0.785      0.831    364.0\n",
      "1.0           0.166      0.667      0.832    364.1\n",
      "10.0          0.267      0.465      0.832    364.0\n",
      "\n",
      "Reference: Retrain AUC=0.523, Advantage=0.046\n",
      "Reference: Baseline AUC=0.783, Advantage=0.565\n"
     ]
    }
   ],
   "source": [
    "# Aggregate gamma=1.0 seeds\n",
    "gamma1_aucs = []\n",
    "gamma1_advantages = []\n",
    "for seed in SEEDS:\n",
    "    key = f'gamma1.0_seed{seed}'\n",
    "    if key in eval_results:\n",
    "        gamma1_aucs.append(eval_results[key]['privacy']['mlp_auc'])\n",
    "        gamma1_advantages.append(eval_results[key]['privacy']['mlp_advantage'])\n",
    "\n",
    "print('Contrastive Results (gamma=1.0, 3 seeds):')\n",
    "print(f'  AUC: {np.mean(gamma1_aucs):.3f} +/- {np.std(gamma1_aucs):.3f}')\n",
    "print(f'  Advantage: {np.mean(gamma1_advantages):.3f} +/- {np.std(gamma1_advantages):.3f}')\n",
    "print()\n",
    "\n",
    "print('Gamma sweep (seed=42):')\n",
    "print(f'{\"Gamma\":<10} {\"AUC\":>8} {\"Advantage\":>10} {\"Marker r\":>10} {\"ELBO\":>8}')\n",
    "print('-' * 50)\n",
    "for gamma in [0.1, 1.0, 10.0]:\n",
    "    key = f'gamma{gamma}_seed42'\n",
    "    if key in eval_results:\n",
    "        r = eval_results[key]\n",
    "        print(f'{gamma:<10.1f} {r[\"privacy\"][\"mlp_auc\"]:>8.3f} '\n",
    "              f'{r[\"privacy\"][\"mlp_advantage\"]:>10.3f} '\n",
    "              f'{r[\"utility\"][\"marker_r\"]:>10.3f} '\n",
    "              f'{r[\"utility\"][\"elbo\"]:>8.1f}')\n",
    "\n",
    "print()\n",
    "print('Reference: Retrain AUC=0.523, Advantage=0.046')\n",
    "print('Reference: Baseline AUC=0.783, Advantage=0.565')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../outputs/p2/contrastive/contrastive_results.json\n"
     ]
    }
   ],
   "source": [
    "output = {\n",
    "    'method': 'contrastive',\n",
    "    'dataset': 'PBMC',\n",
    "    'forget_type': 'structured',\n",
    "    'seeds': SEEDS,\n",
    "    'gamma_sweep': [0.1, 1.0, 10.0],\n",
    "    'results': eval_results,\n",
    "    'summary': {\n",
    "        'gamma1.0': {\n",
    "            'mean_auc': float(np.mean(gamma1_aucs)),\n",
    "            'std_auc': float(np.std(gamma1_aucs)),\n",
    "            'mean_advantage': float(np.mean(gamma1_advantages)),\n",
    "            'std_advantage': float(np.std(gamma1_advantages)),\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(OUTPUT_BASE / 'contrastive_results.json', 'w') as f:\n",
    "    json.dump(output, f, indent=2, default=str)\n",
    "\n",
    "print(f'Saved to {OUTPUT_BASE / \"contrastive_results.json\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7vr125637w",
   "source": "## 4. Analysis\n\nContrastive latent unlearning produces a Streisand effect. At gamma=1.0 across three seeds, MIA AUC drops to 0.153 +/- 0.032, well below the 0.5 chance line. Because the advantage metric is direction-agnostic, this translates to 0.695, worse than the baseline's 0.565. Lower gamma (0.1) makes things worse (AUC=0.107, advantage=0.785). Higher gamma (10.0) partially mitigates the problem (AUC=0.267, advantage=0.465) but still fails.\n\nWhat happens is not surprising in hindsight. Pushing forget-set posteriors toward N(0,I) puts them in a region of latent space that no retain-set sample occupies. The attacker picks up on this immediately: samples near the prior are forget samples. Stronger push, more obvious signal.\n\nUtility is essentially unchanged (marker r=0.831-0.832, ELBO=363.9-364.2). The contrastive phase only modifies the encoder for forget-set inputs and leaves the decoder and retain-set representations alone. Held-out reconstruction is fine because those pathways were never touched.\n\nThe takeaway for the paper: latent-space manipulation without whole-model adjustment is counterproductive. Any method that moves forget samples to a distinctive location in representation space, whether the prior or some other target, creates exactly the artifact that membership inference detects. Information needs to be removed, not relocated.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat4243",
   "language": "python",
   "name": "stat4243"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}