{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 23. DP-SGD Baseline Experiments\n",
    "\n",
    "DP-SGD (Abadi et al., 2016) trains a VAE from scratch on the retain set\n",
    "with differential privacy guarantees via per-sample gradient clipping and\n",
    "Gaussian noise injection. This provides a formal privacy baseline:\n",
    "the forget set was never in training, so membership inference should be\n",
    "at chance. The utility cost of DP noise establishes a lower bound on\n",
    "what privacy guarantees cost.\n",
    "\n",
    "We test epsilon in {1, 10, 50} to measure the privacy-utility tradeoff.\n",
    "Uses Opacus for DP-SGD implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "from train_dp import train_dp\n",
    "\n",
    "DATA_PATH = '../data/adata_processed.h5ad'\n",
    "SPLIT_PATH = '../outputs/p1/split_structured.json'\n",
    "OUTPUT_BASE = Path('../outputs/p2/dp_sgd')\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Train DP-SGD VAE at Multiple Epsilon Values\n",
    "\n",
    "Parameters:\n",
    "- target_delta = 1e-5 (< 1/n for n~28k retain samples)\n",
    "- max_grad_norm = 1.0\n",
    "- 50 epochs maximum, patience=15\n",
    "- Opacus auto-calibrates noise_multiplier for target epsilon\n",
    "\n",
    "Note: DP-SGD trains from scratch (no baseline checkpoint needed).\n",
    "This is slower than post-hoc methods but provides formal guarantees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DP-SGD epsilon=1.0, seed=42\n",
      "============================================================\n",
      "Data: torch.Size([33088, 2000]), Retain: 28094, Device: cpu\n",
      "Target epsilon: 1.0, delta: 1e-05\n",
      "Creating DP-compatible VAE (2000 -> [1024, 512, 128] -> z=32)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dbenson/.pyenv/versions/stat4243/lib/python3.12/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrated noise_multiplier: 2.8840 (target_eps=1.0, steps=5250)\n",
      "Training for up to 50 epochs (~105 batches/epoch)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dbenson/.pyenv/versions/stat4243/lib/python3.12/site-packages/opacus/privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/dbenson/.pyenv/versions/stat4243/lib/python3.12/site-packages/torch/nn/modules/module.py:1352: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1: train=767.03, val=600.79, eps=0.14, time=326s\n",
      "  Epoch 5: train=559.22, val=505.91, eps=0.30, time=311s\n",
      "  Epoch 10: train=539.51, val=468.51, eps=0.43, time=347s\n",
      "  Epoch 15: train=544.76, val=462.86, eps=0.53, time=363s\n",
      "  Epoch 20: train=552.00, val=461.25, eps=0.61, time=296s\n",
      "  Epoch 25: train=554.71, val=461.63, eps=0.69, time=294s\n",
      "  Epoch 30: train=554.88, val=460.99, eps=0.76, time=298s\n",
      "  Epoch 35: train=550.93, val=460.14, eps=0.83, time=294s\n",
      "  Epoch 40: train=546.67, val=458.10, eps=0.89, time=294s\n",
      "  Epoch 45: train=542.05, val=456.93, eps=0.95, time=290s\n",
      "  Epoch 50: train=536.76, val=456.93, eps=1.00, time=294s\n",
      "\n",
      "Final privacy budget: epsilon=1.00, delta=1e-05\n",
      "Saved to ../outputs/p2/dp_sgd/eps1.0_seed42/best_model.pt\n",
      "Done in 15347.7s\n",
      "\n",
      "============================================================\n",
      "DP-SGD epsilon=10.0, seed=42\n",
      "============================================================\n",
      "Data: torch.Size([33088, 2000]), Retain: 28094, Device: cpu\n",
      "Target epsilon: 10.0, delta: 1e-05\n",
      "Creating DP-compatible VAE (2000 -> [1024, 512, 128] -> z=32)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dbenson/.pyenv/versions/stat4243/lib/python3.12/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrated noise_multiplier: 0.7102 (target_eps=10.0, steps=5250)\n",
      "Training for up to 50 epochs (~105 batches/epoch)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dbenson/.pyenv/versions/stat4243/lib/python3.12/site-packages/opacus/privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/dbenson/.pyenv/versions/stat4243/lib/python3.12/site-packages/torch/nn/modules/module.py:1352: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1: train=614.24, val=495.84, eps=2.96, time=302s\n",
      "  Epoch 5: train=478.36, val=436.45, eps=4.08, time=292s\n",
      "  Epoch 10: train=463.00, val=416.63, eps=5.04, time=295s\n",
      "  Epoch 15: train=453.15, val=411.18, eps=5.85, time=292s\n",
      "  Epoch 20: train=447.69, val=408.33, eps=6.57, time=291s\n",
      "  Epoch 25: train=441.68, val=406.57, eps=7.23, time=293s\n",
      "  Epoch 30: train=438.78, val=404.68, eps=7.84, time=298s\n",
      "  Epoch 35: train=434.09, val=403.50, eps=8.43, time=295s\n",
      "  Epoch 40: train=431.43, val=402.86, eps=8.98, time=291s\n",
      "  Epoch 45: train=429.80, val=401.75, eps=9.51, time=289s\n",
      "  Epoch 50: train=428.05, val=400.29, eps=10.03, time=294s\n",
      "\n",
      "Final privacy budget: epsilon=10.03, delta=1e-05\n",
      "Saved to ../outputs/p2/dp_sgd/eps10.0_seed42/best_model.pt\n",
      "Done in 14705.9s\n",
      "\n",
      "============================================================\n",
      "DP-SGD epsilon=50.0, seed=42\n",
      "============================================================\n",
      "Data: torch.Size([33088, 2000]), Retain: 28094, Device: cpu\n",
      "Target epsilon: 50.0, delta: 1e-05\n",
      "Creating DP-compatible VAE (2000 -> [1024, 512, 128] -> z=32)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dbenson/.pyenv/versions/stat4243/lib/python3.12/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrated noise_multiplier: 0.4374 (target_eps=50.0, steps=5250)\n",
      "Training for up to 50 epochs (~105 batches/epoch)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dbenson/.pyenv/versions/stat4243/lib/python3.12/site-packages/opacus/privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/dbenson/.pyenv/versions/stat4243/lib/python3.12/site-packages/torch/nn/modules/module.py:1352: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1: train=578.58, val=465.78, eps=11.46, time=296s\n",
      "  Epoch 5: train=454.68, val=416.33, eps=18.01, time=293s\n",
      "  Epoch 10: train=440.20, val=405.58, eps=23.33, time=294s\n",
      "  Epoch 15: train=432.50, val=402.37, eps=27.59, time=291s\n",
      "  Epoch 20: train=428.58, val=399.94, eps=31.42, time=293s\n",
      "  Epoch 25: train=424.13, val=398.33, eps=34.92, time=293s\n",
      "  Epoch 30: train=422.31, val=396.75, eps=38.42, time=296s\n",
      "  Epoch 35: train=418.95, val=395.73, eps=41.47, time=294s\n",
      "  Epoch 40: train=417.26, val=395.30, eps=44.38, time=294s\n",
      "  Epoch 45: train=416.36, val=394.42, eps=47.29, time=288s\n",
      "  Epoch 50: train=415.10, val=393.44, eps=50.20, time=292s\n",
      "\n",
      "Final privacy budget: epsilon=50.20, delta=1e-05\n",
      "Saved to ../outputs/p2/dp_sgd/eps50.0_seed42/best_model.pt\n",
      "Done in 14688.6s\n",
      "\n",
      "============================================================\n",
      "DP-SGD epsilon=10.0, seed=123\n",
      "============================================================\n",
      "Data: torch.Size([33088, 2000]), Retain: 28094, Device: cpu\n",
      "Target epsilon: 10.0, delta: 1e-05\n",
      "Creating DP-compatible VAE (2000 -> [1024, 512, 128] -> z=32)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dbenson/.pyenv/versions/stat4243/lib/python3.12/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrated noise_multiplier: 0.7102 (target_eps=10.0, steps=5250)\n",
      "Training for up to 50 epochs (~105 batches/epoch)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dbenson/.pyenv/versions/stat4243/lib/python3.12/site-packages/opacus/privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/dbenson/.pyenv/versions/stat4243/lib/python3.12/site-packages/torch/nn/modules/module.py:1352: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1: train=612.48, val=505.27, eps=2.96, time=299s\n",
      "  Epoch 5: train=479.17, val=445.11, eps=4.08, time=294s\n",
      "  Epoch 10: train=467.56, val=425.77, eps=5.04, time=302s\n",
      "  Epoch 15: train=456.04, val=415.14, eps=5.85, time=300s\n",
      "  Epoch 20: train=448.05, val=411.61, eps=6.57, time=361s\n",
      "  Epoch 25: train=441.64, val=408.82, eps=7.23, time=298s\n",
      "  Epoch 30: train=437.01, val=407.40, eps=7.84, time=294s\n",
      "  Epoch 35: train=433.55, val=406.02, eps=8.43, time=293s\n",
      "  Epoch 40: train=431.15, val=404.39, eps=8.98, time=292s\n",
      "  Epoch 45: train=429.33, val=403.50, eps=9.51, time=298s\n",
      "  Epoch 50: train=426.97, val=402.38, eps=10.03, time=294s\n",
      "\n",
      "Final privacy budget: epsilon=10.03, delta=1e-05\n",
      "Saved to ../outputs/p2/dp_sgd/eps10.0_seed123/best_model.pt\n",
      "Done in 15330.7s\n",
      "\n",
      "============================================================\n",
      "DP-SGD epsilon=10.0, seed=456\n",
      "============================================================\n",
      "Data: torch.Size([33088, 2000]), Retain: 28094, Device: cpu\n",
      "Target epsilon: 10.0, delta: 1e-05\n",
      "Creating DP-compatible VAE (2000 -> [1024, 512, 128] -> z=32)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dbenson/.pyenv/versions/stat4243/lib/python3.12/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrated noise_multiplier: 0.7102 (target_eps=10.0, steps=5250)\n",
      "Training for up to 50 epochs (~105 batches/epoch)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dbenson/.pyenv/versions/stat4243/lib/python3.12/site-packages/opacus/privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/dbenson/.pyenv/versions/stat4243/lib/python3.12/site-packages/torch/nn/modules/module.py:1352: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1: train=614.92, val=508.14, eps=2.96, time=303s\n",
      "  Epoch 5: train=474.22, val=438.97, eps=4.08, time=294s\n",
      "  Epoch 10: train=459.26, val=419.27, eps=5.04, time=297s\n",
      "  Epoch 15: train=453.24, val=415.03, eps=5.85, time=293s\n",
      "  Epoch 20: train=445.09, val=412.33, eps=6.57, time=294s\n",
      "  Epoch 25: train=439.71, val=409.20, eps=7.23, time=293s\n",
      "  Epoch 30: train=434.61, val=408.08, eps=7.84, time=294s\n",
      "  Epoch 35: train=432.56, val=406.24, eps=8.43, time=294s\n",
      "  Epoch 40: train=430.11, val=405.50, eps=8.98, time=291s\n",
      "  Epoch 45: train=426.60, val=404.40, eps=9.51, time=301s\n",
      "  Epoch 50: train=425.19, val=403.75, eps=10.03, time=302s\n",
      "\n",
      "Final privacy budget: epsilon=10.03, delta=1e-05\n",
      "Saved to ../outputs/p2/dp_sgd/eps10.0_seed456/best_model.pt\n",
      "Done in 14816.5s\n",
      "\n",
      "All training complete. 5 checkpoints saved.\n"
     ]
    }
   ],
   "source": [
    "EPSILONS = [1.0, 10.0, 50.0]\n",
    "SEEDS = [42, 123, 456]\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Epsilon sweep with seed=42\n",
    "for eps in EPSILONS:\n",
    "    out_dir = OUTPUT_BASE / f'eps{eps}_seed42'\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'DP-SGD epsilon={eps}, seed=42')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    t0 = time.time()\n",
    "    ckpt_path = train_dp(\n",
    "        data_path=DATA_PATH,\n",
    "        split_path=SPLIT_PATH,\n",
    "        output_dir=str(out_dir),\n",
    "        target_epsilon=eps,\n",
    "        target_delta=1e-5,\n",
    "        max_grad_norm=1.0,\n",
    "        n_epochs=50,\n",
    "        lr=1e-3,\n",
    "        batch_size=256,\n",
    "        latent_dim=32,\n",
    "        hidden_dims=[1024, 512, 128],\n",
    "        patience=15,\n",
    "        seed=42,\n",
    "    )\n",
    "    elapsed = time.time() - t0\n",
    "    results[f'eps{eps}_seed42'] = {'path': str(ckpt_path), 'time': elapsed}\n",
    "    print(f'Done in {elapsed:.1f}s')\n",
    "\n",
    "# Multi-seed for eps=10 (our main comparison point)\n",
    "for seed in [123, 456]:\n",
    "    out_dir = OUTPUT_BASE / f'eps10.0_seed{seed}'\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'DP-SGD epsilon=10.0, seed={seed}')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    t0 = time.time()\n",
    "    ckpt_path = train_dp(\n",
    "        data_path=DATA_PATH,\n",
    "        split_path=SPLIT_PATH,\n",
    "        output_dir=str(out_dir),\n",
    "        target_epsilon=10.0,\n",
    "        target_delta=1e-5,\n",
    "        max_grad_norm=1.0,\n",
    "        n_epochs=50,\n",
    "        lr=1e-3,\n",
    "        batch_size=256,\n",
    "        latent_dim=32,\n",
    "        hidden_dims=[1024, 512, 128],\n",
    "        patience=15,\n",
    "        seed=seed,\n",
    "    )\n",
    "    elapsed = time.time() - t0\n",
    "    results[f'eps10.0_seed{seed}'] = {'path': str(ckpt_path), 'time': elapsed}\n",
    "    print(f'Done in {elapsed:.1f}s')\n",
    "\n",
    "print(f'\\nAll training complete. {len(results)} checkpoints saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Evaluate with Canonical Fresh Attacker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fresh attacker on baseline F vs matched:\n",
      "  Samples: 224 (30 forget + 194 matched)\n",
      "  Features: 70 dims\n",
      "  Train: 179, Test: 45\n",
      "  Baseline AUC (F vs matched, full set): 0.7792\n",
      "  (Canonical NB03 value: ~0.769)\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, '../scripts')\n",
    "from eval_multiseed import (\n",
    "    load_vae_model, train_fresh_attacker, evaluate_privacy,\n",
    "    evaluate_utility, MARKER_GENES\n",
    ")\n",
    "import scanpy as sc\n",
    "\n",
    "adata = sc.read_h5ad(DATA_PATH)\n",
    "X = torch.tensor(\n",
    "    adata.X.toarray() if hasattr(adata.X, 'toarray') else adata.X,\n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "with open(SPLIT_PATH) as f:\n",
    "    split = json.load(f)\n",
    "forget_idx = split['forget_indices']\n",
    "retain_idx = split['retain_indices']\n",
    "unseen_idx = split['unseen_indices']\n",
    "\n",
    "with open('../outputs/p1.5/s1_matched_negatives.json') as f:\n",
    "    matched_data = json.load(f)\n",
    "matched_neg_idx = matched_data['matched_indices']\n",
    "\n",
    "X_holdout = X[unseen_idx]\n",
    "labels_holdout = adata.obs['leiden'].values[unseen_idx]\n",
    "gene_names = list(adata.var_names)\n",
    "marker_idx = [gene_names.index(g) for g in MARKER_GENES if g in gene_names]\n",
    "marker_names = [g for g in MARKER_GENES if g in gene_names]\n",
    "\n",
    "# Train fresh attacker on baseline\n",
    "BASELINE_CKPT = '../outputs/p1/baseline/best_model.pt'\n",
    "baseline_model, _ = load_vae_model(BASELINE_CKPT)\n",
    "attacker = train_fresh_attacker(\n",
    "    baseline_model, adata, forget_idx, matched_neg_idx, retain_idx, seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating eps1.0_seed42...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dbenson/.pyenv/versions/stat4243/lib/python3.12/site-packages/threadpoolctl.py:1226: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  AUC=0.646, Advantage=0.292, ELBO=461.4, Marker r=0.495, eps=1.0035028714928582\n",
      "\n",
      "Evaluating eps10.0_seed42...\n",
      "  AUC=0.472, Advantage=0.056, ELBO=404.4, Marker r=0.781, eps=10.030516019139084\n",
      "\n",
      "Evaluating eps50.0_seed42...\n",
      "  AUC=0.438, Advantage=0.123, ELBO=397.2, Marker r=0.795, eps=50.19651091262326\n",
      "\n",
      "Evaluating eps10.0_seed123...\n",
      "  AUC=0.431, Advantage=0.137, ELBO=402.8, Marker r=0.790, eps=10.030516019139084\n",
      "\n",
      "Evaluating eps10.0_seed456...\n",
      "  AUC=0.488, Advantage=0.023, ELBO=402.7, Marker r=0.789, eps=10.030516019139084\n"
     ]
    }
   ],
   "source": [
    "eval_results = {}\n",
    "\n",
    "for name, info in results.items():\n",
    "    ckpt_path = info['path']\n",
    "    print(f'\\nEvaluating {name}...')\n",
    "    \n",
    "    model, config = load_vae_model(ckpt_path)\n",
    "    \n",
    "    privacy = evaluate_privacy(\n",
    "        model, attacker, adata, forget_idx, matched_neg_idx, retain_idx\n",
    "    )\n",
    "    utility = evaluate_utility(\n",
    "        model, X_holdout, labels_holdout, marker_idx, gene_names\n",
    "    )\n",
    "    \n",
    "    # Load achieved epsilon from checkpoint\n",
    "    ckpt = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "    achieved_eps = ckpt.get('achieved_epsilon', 'unknown')\n",
    "    \n",
    "    eval_results[name] = {\n",
    "        'privacy': privacy,\n",
    "        'utility': utility,\n",
    "        'training_time': info['time'],\n",
    "        'achieved_epsilon': achieved_eps,\n",
    "    }\n",
    "    \n",
    "    print(f'  AUC={privacy[\"mlp_auc\"]:.3f}, '\n",
    "          f'Advantage={privacy[\"mlp_advantage\"]:.3f}, '\n",
    "          f'ELBO={utility[\"elbo\"]:.1f}, '\n",
    "          f'Marker r={utility[\"marker_r\"]:.3f}, '\n",
    "          f'eps={achieved_eps}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Summary: Privacy-Utility Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP-SGD Epsilon Sweep (seed=42):\n",
      "Epsilon         AUC  Advantage   Marker r     ELBO Achieved eps\n",
      "--------------------------------------------------------------\n",
      "1.0           0.646      0.292      0.495    461.4 1.0035028714928582\n",
      "10.0          0.472      0.056      0.781    404.4 10.030516019139084\n",
      "50.0          0.438      0.123      0.795    397.2 50.19651091262326\n",
      "\n",
      "DP-SGD eps=10 (3 seeds):\n",
      "  AUC: 0.464 +/- 0.024\n",
      "  Advantage: 0.072 +/- 0.048\n",
      "\n",
      "Reference: Retrain AUC=0.523, Advantage=0.046\n",
      "Reference: Baseline AUC=0.783, Advantage=0.565\n",
      "\n",
      "Note: DP-SGD trains from scratch on retain set, so forget set\n",
      "was never seen. MIA AUC should be near chance (0.5) unless the\n",
      "DP noise degrades the model so much that it creates artifacts.\n"
     ]
    }
   ],
   "source": [
    "# Epsilon sweep comparison\n",
    "print('DP-SGD Epsilon Sweep (seed=42):')\n",
    "print(f'{\"Epsilon\":<10} {\"AUC\":>8} {\"Advantage\":>10} {\"Marker r\":>10} {\"ELBO\":>8} {\"Achieved eps\":>12}')\n",
    "print('-' * 62)\n",
    "for eps in EPSILONS:\n",
    "    key = f'eps{eps}_seed42'\n",
    "    if key in eval_results:\n",
    "        r = eval_results[key]\n",
    "        print(f'{eps:<10.1f} {r[\"privacy\"][\"mlp_auc\"]:>8.3f} '\n",
    "              f'{r[\"privacy\"][\"mlp_advantage\"]:>10.3f} '\n",
    "              f'{r[\"utility\"][\"marker_r\"]:>10.3f} '\n",
    "              f'{r[\"utility\"][\"elbo\"]:>8.1f} '\n",
    "              f'{r[\"achieved_epsilon\"]:>12}')\n",
    "\n",
    "print()\n",
    "\n",
    "# Eps=10 multi-seed\n",
    "eps10_aucs = []\n",
    "eps10_advantages = []\n",
    "for seed in SEEDS:\n",
    "    key = f'eps10.0_seed{seed}'\n",
    "    if key in eval_results:\n",
    "        eps10_aucs.append(eval_results[key]['privacy']['mlp_auc'])\n",
    "        eps10_advantages.append(eval_results[key]['privacy']['mlp_advantage'])\n",
    "\n",
    "if eps10_aucs:\n",
    "    print(f'DP-SGD eps=10 (3 seeds):')\n",
    "    print(f'  AUC: {np.mean(eps10_aucs):.3f} +/- {np.std(eps10_aucs):.3f}')\n",
    "    print(f'  Advantage: {np.mean(eps10_advantages):.3f} +/- {np.std(eps10_advantages):.3f}')\n",
    "\n",
    "print()\n",
    "print('Reference: Retrain AUC=0.523, Advantage=0.046')\n",
    "print('Reference: Baseline AUC=0.783, Advantage=0.565')\n",
    "print()\n",
    "print('Note: DP-SGD trains from scratch on retain set, so forget set')\n",
    "print('was never seen. MIA AUC should be near chance (0.5) unless the')\n",
    "print('DP noise degrades the model so much that it creates artifacts.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../outputs/p2/dp_sgd/dp_sgd_results.json\n"
     ]
    }
   ],
   "source": [
    "output = {\n",
    "    'method': 'dp_sgd',\n",
    "    'dataset': 'PBMC',\n",
    "    'forget_type': 'structured',\n",
    "    'seeds': SEEDS,\n",
    "    'epsilon_sweep': EPSILONS,\n",
    "    'results': eval_results,\n",
    "    'summary': {\n",
    "        'eps10': {\n",
    "            'mean_auc': float(np.mean(eps10_aucs)) if eps10_aucs else None,\n",
    "            'std_auc': float(np.std(eps10_aucs)) if eps10_aucs else None,\n",
    "            'mean_advantage': float(np.mean(eps10_advantages)) if eps10_advantages else None,\n",
    "            'std_advantage': float(np.std(eps10_advantages)) if eps10_advantages else None,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(OUTPUT_BASE / 'dp_sgd_results.json', 'w') as f:\n",
    "    json.dump(output, f, indent=2, default=str)\n",
    "\n",
    "print(f'Saved to {OUTPUT_BASE / \"dp_sgd_results.json\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "le9rfzgw7t",
   "source": "## 4. Analysis\n\nDP-SGD is the only method tested that gets close to retrain-equivalent privacy. At eps=10 across three seeds, MIA AUC is 0.464 +/- 0.024 with advantage 0.072 +/- 0.048. One seed (456) hits advantage=0.023, below the retrain threshold of 0.046. The mean sits slightly above it, but it is in the right neighborhood.\n\nThe epsilon sweep shows a non-monotonic privacy-utility tradeoff. At eps=1.0, the noise is so heavy that the model falls apart (marker r=0.495, ELBO=461), and that damage itself becomes detectable (AUC=0.646, advantage=0.292). The model is so distorted that forget and retain samples produce distinguishable outputs, but for reasons unrelated to memorization. At eps=10.0, the noise lands in a useful range: enough to mask membership signals, little enough for the model to still learn structure (marker r=0.781, ELBO=404). At eps=50.0, less noise gives slightly better utility (marker r=0.795, ELBO=397) but the model over-unlearns slightly (AUC=0.438, advantage=0.123).\n\nDP-SGD \"works\" because the forget set was never in training. That is the whole explanation. No post-hoc method tested in this study came close to matching it, and DP-SGD itself requires accepting a real utility penalty (marker r 0.781 vs 0.832, ELBO 404 vs 364).",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat4243",
   "language": "python",
   "name": "stat4243"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}