{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 24. SCRUB Experiments\n",
    "\n",
    "SCRUB (Kurmanji et al., NeurIPS 2023) uses teacher-student distillation:\n",
    "the student matches the teacher on retain data and diverges from the\n",
    "teacher on forget data. This is the state-of-the-art in machine unlearning.\n",
    "\n",
    "Adaptation for VAEs:\n",
    "- Teacher = frozen baseline VAE\n",
    "- Student = copy of baseline (being updated)\n",
    "- Forget: maximize KL(student_posterior || teacher_posterior)\n",
    "- Retain: minimize KL(student_posterior || teacher_posterior) + ELBO\n",
    "- Alternating optimization (forget steps, then retain steps)\n",
    "\n",
    "Tested on PBMC structured forget set with 3 seeds + alpha sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "from train_scrub import train_scrub\n",
    "\n",
    "DATA_PATH = '../data/adata_processed.h5ad'\n",
    "SPLIT_PATH = '../outputs/p1/split_structured.json'\n",
    "BASELINE_CKPT = '../outputs/p1/baseline/best_model.pt'\n",
    "OUTPUT_BASE = Path('../outputs/p2/scrub')\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Train SCRUB with 3 Seeds\n",
    "\n",
    "Hyperparameters:\n",
    "- alpha_forget=1.0 (forget divergence weight)\n",
    "- alpha_retain=1.0 (retain matching weight)\n",
    "- 20 SCRUB epochs, 5 forget + 10 retain steps per epoch\n",
    "- lr_forget=1e-4, lr_retain=1e-4\n",
    "- max_grad_norm=1.0\n",
    "- 10 retain fine-tuning epochs\n",
    "\n",
    "Sweep alpha_forget in {0.1, 1.0, 10.0} with seed=42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SCRUB alpha_forget=1.0, seed=42\n",
      "============================================================\n",
      "Data: torch.Size([33088, 2000]), Forget: 30, Retain: 28094, Device: cpu\n",
      "SCRUB training: 20 epochs, 5 forget + 10 retain steps/epoch\n",
      "  alpha_forget=1.0, alpha_retain=1.0\n",
      "  Epoch 1: forget=-8.7694, retain=368.8553\n",
      "  Epoch 5: forget=-56.8306, retain=372.3197\n",
      "  Epoch 10: forget=-178.4803, retain=372.8519\n",
      "  Epoch 15: forget=-347.6720, retain=376.4480\n",
      "  Epoch 20: forget=-515.9554, retain=375.1795\n",
      "Fine-tuning on retain set (10 epochs, lr=0.0001)...\n",
      "  Epoch 1: train=366.38, val=358.01\n",
      "  Epoch 5: train=365.63, val=357.77\n",
      "  Epoch 10: train=365.31, val=357.63\n",
      "Saved to ../outputs/p2/scrub/af1.0_seed42/best_model.pt\n",
      "Done in 81.5s\n",
      "\n",
      "============================================================\n",
      "SCRUB alpha_forget=1.0, seed=123\n",
      "============================================================\n",
      "Data: torch.Size([33088, 2000]), Forget: 30, Retain: 28094, Device: cpu\n",
      "SCRUB training: 20 epochs, 5 forget + 10 retain steps/epoch\n",
      "  alpha_forget=1.0, alpha_retain=1.0\n",
      "  Epoch 1: forget=-6.3483, retain=368.5155\n",
      "  Epoch 5: forget=-53.6064, retain=370.8502\n",
      "  Epoch 10: forget=-181.1074, retain=374.7687\n",
      "  Epoch 15: forget=-354.9250, retain=372.1782\n",
      "  Epoch 20: forget=-542.1654, retain=375.1828\n",
      "Fine-tuning on retain set (10 epochs, lr=0.0001)...\n",
      "  Epoch 1: train=366.40, val=361.84\n",
      "  Epoch 5: train=365.59, val=361.51\n",
      "  Epoch 10: train=365.31, val=361.16\n",
      "Saved to ../outputs/p2/scrub/af1.0_seed123/best_model.pt\n",
      "Done in 85.7s\n",
      "\n",
      "============================================================\n",
      "SCRUB alpha_forget=1.0, seed=456\n",
      "============================================================\n",
      "Data: torch.Size([33088, 2000]), Forget: 30, Retain: 28094, Device: cpu\n",
      "SCRUB training: 20 epochs, 5 forget + 10 retain steps/epoch\n",
      "  alpha_forget=1.0, alpha_retain=1.0\n",
      "  Epoch 1: forget=-8.7131, retain=370.6025\n",
      "  Epoch 5: forget=-52.5518, retain=370.0776\n",
      "  Epoch 10: forget=-141.0221, retain=377.8489\n",
      "  Epoch 15: forget=-277.2206, retain=375.8367\n",
      "  Epoch 20: forget=-424.7560, retain=375.9954\n",
      "Fine-tuning on retain set (10 epochs, lr=0.0001)...\n",
      "  Epoch 1: train=366.66, val=362.61\n",
      "  Epoch 5: train=365.63, val=362.02\n",
      "  Epoch 10: train=365.40, val=361.87\n",
      "Saved to ../outputs/p2/scrub/af1.0_seed456/best_model.pt\n",
      "Done in 98.1s\n",
      "\n",
      "============================================================\n",
      "SCRUB alpha_forget=0.1, seed=42\n",
      "============================================================\n",
      "Data: torch.Size([33088, 2000]), Forget: 30, Retain: 28094, Device: cpu\n",
      "SCRUB training: 20 epochs, 5 forget + 10 retain steps/epoch\n",
      "  alpha_forget=0.1, alpha_retain=1.0\n",
      "  Epoch 1: forget=-0.8769, retain=368.8553\n",
      "  Epoch 5: forget=-5.6829, retain=372.3191\n",
      "  Epoch 10: forget=-17.8655, retain=372.8477\n",
      "  Epoch 15: forget=-34.8190, retain=376.4198\n",
      "  Epoch 20: forget=-51.7406, retain=375.1940\n",
      "Fine-tuning on retain set (10 epochs, lr=0.0001)...\n",
      "  Epoch 1: train=366.38, val=358.00\n",
      "  Epoch 5: train=365.63, val=357.76\n",
      "  Epoch 10: train=365.30, val=357.62\n",
      "Saved to ../outputs/p2/scrub/af0.1_seed42/best_model.pt\n",
      "Done in 89.6s\n",
      "\n",
      "============================================================\n",
      "SCRUB alpha_forget=10.0, seed=42\n",
      "============================================================\n",
      "Data: torch.Size([33088, 2000]), Forget: 30, Retain: 28094, Device: cpu\n",
      "SCRUB training: 20 epochs, 5 forget + 10 retain steps/epoch\n",
      "  alpha_forget=10.0, alpha_retain=1.0\n",
      "  Epoch 1: forget=-87.6941, retain=368.8553\n",
      "  Epoch 5: forget=-568.4129, retain=372.3207\n",
      "  Epoch 10: forget=-1784.6703, retain=372.8236\n",
      "  Epoch 15: forget=-3480.9650, retain=376.4214\n",
      "  Epoch 20: forget=-5182.0568, retain=375.1969\n",
      "Fine-tuning on retain set (10 epochs, lr=0.0001)...\n",
      "  Epoch 1: train=366.38, val=358.00\n",
      "  Epoch 5: train=365.63, val=357.77\n",
      "  Epoch 10: train=365.30, val=357.62\n",
      "Saved to ../outputs/p2/scrub/af10.0_seed42/best_model.pt\n",
      "Done in 93.1s\n",
      "\n",
      "All training complete. 5 checkpoints saved.\n"
     ]
    }
   ],
   "source": [
    "SEEDS = [42, 123, 456]\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Multi-seed with default alpha_forget=1.0\n",
    "for seed in SEEDS:\n",
    "    out_dir = OUTPUT_BASE / f'af1.0_seed{seed}'\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'SCRUB alpha_forget=1.0, seed={seed}')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    t0 = time.time()\n",
    "    ckpt_path = train_scrub(\n",
    "        baseline_checkpoint=BASELINE_CKPT,\n",
    "        data_path=DATA_PATH,\n",
    "        split_path=SPLIT_PATH,\n",
    "        output_dir=str(out_dir),\n",
    "        alpha_forget=1.0,\n",
    "        alpha_retain=1.0,\n",
    "        n_epochs=20,\n",
    "        forget_steps_per_epoch=5,\n",
    "        retain_steps_per_epoch=10,\n",
    "        lr_forget=1e-4,\n",
    "        lr_retain=1e-4,\n",
    "        max_grad_norm=1.0,\n",
    "        finetune_epochs=10,\n",
    "        finetune_lr=1e-4,\n",
    "        patience=10,\n",
    "        batch_size=256,\n",
    "        seed=seed,\n",
    "    )\n",
    "    elapsed = time.time() - t0\n",
    "    results[f'af1.0_seed{seed}'] = {'path': str(ckpt_path), 'time': elapsed}\n",
    "    print(f'Done in {elapsed:.1f}s')\n",
    "\n",
    "# Alpha_forget sweep with seed=42\n",
    "for af in [0.1, 10.0]:\n",
    "    out_dir = OUTPUT_BASE / f'af{af}_seed42'\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'SCRUB alpha_forget={af}, seed=42')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    t0 = time.time()\n",
    "    ckpt_path = train_scrub(\n",
    "        baseline_checkpoint=BASELINE_CKPT,\n",
    "        data_path=DATA_PATH,\n",
    "        split_path=SPLIT_PATH,\n",
    "        output_dir=str(out_dir),\n",
    "        alpha_forget=af,\n",
    "        alpha_retain=1.0,\n",
    "        n_epochs=20,\n",
    "        forget_steps_per_epoch=5,\n",
    "        retain_steps_per_epoch=10,\n",
    "        lr_forget=1e-4,\n",
    "        lr_retain=1e-4,\n",
    "        max_grad_norm=1.0,\n",
    "        finetune_epochs=10,\n",
    "        finetune_lr=1e-4,\n",
    "        patience=10,\n",
    "        batch_size=256,\n",
    "        seed=42,\n",
    "    )\n",
    "    elapsed = time.time() - t0\n",
    "    results[f'af{af}_seed42'] = {'path': str(ckpt_path), 'time': elapsed}\n",
    "    print(f'Done in {elapsed:.1f}s')\n",
    "\n",
    "print(f'\\nAll training complete. {len(results)} checkpoints saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Evaluate with Canonical Fresh Attacker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fresh attacker on baseline F vs matched:\n",
      "  Samples: 224 (30 forget + 194 matched)\n",
      "  Features: 70 dims\n",
      "  Train: 179, Test: 45\n",
      "  Baseline AUC (F vs matched, full set): 0.7792\n",
      "  (Canonical NB03 value: ~0.769)\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, '../scripts')\n",
    "from eval_multiseed import (\n",
    "    load_vae_model, train_fresh_attacker, evaluate_privacy,\n",
    "    evaluate_utility, MARKER_GENES\n",
    ")\n",
    "import scanpy as sc\n",
    "\n",
    "adata = sc.read_h5ad(DATA_PATH)\n",
    "X = torch.tensor(\n",
    "    adata.X.toarray() if hasattr(adata.X, 'toarray') else adata.X,\n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "with open(SPLIT_PATH) as f:\n",
    "    split = json.load(f)\n",
    "forget_idx = split['forget_indices']\n",
    "retain_idx = split['retain_indices']\n",
    "unseen_idx = split['unseen_indices']\n",
    "\n",
    "with open('../outputs/p1.5/s1_matched_negatives.json') as f:\n",
    "    matched_data = json.load(f)\n",
    "matched_neg_idx = matched_data['matched_indices']\n",
    "\n",
    "X_holdout = X[unseen_idx]\n",
    "labels_holdout = adata.obs['leiden'].values[unseen_idx]\n",
    "gene_names = list(adata.var_names)\n",
    "marker_idx = [gene_names.index(g) for g in MARKER_GENES if g in gene_names]\n",
    "marker_names = [g for g in MARKER_GENES if g in gene_names]\n",
    "\n",
    "baseline_model, _ = load_vae_model(BASELINE_CKPT)\n",
    "attacker = train_fresh_attacker(\n",
    "    baseline_model, adata, forget_idx, matched_neg_idx, retain_idx, seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating af1.0_seed42...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dbenson/.pyenv/versions/stat4243/lib/python3.12/site-packages/threadpoolctl.py:1226: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  AUC=0.736, Advantage=0.473, ELBO=363.8, Marker r=0.832\n",
      "\n",
      "Evaluating af1.0_seed123...\n",
      "  AUC=0.735, Advantage=0.471, ELBO=363.8, Marker r=0.831\n",
      "\n",
      "Evaluating af1.0_seed456...\n",
      "  AUC=0.739, Advantage=0.479, ELBO=363.8, Marker r=0.831\n",
      "\n",
      "Evaluating af0.1_seed42...\n",
      "  AUC=0.731, Advantage=0.461, ELBO=363.8, Marker r=0.832\n",
      "\n",
      "Evaluating af10.0_seed42...\n",
      "  AUC=0.732, Advantage=0.463, ELBO=363.8, Marker r=0.832\n"
     ]
    }
   ],
   "source": [
    "eval_results = {}\n",
    "\n",
    "for name, info in results.items():\n",
    "    ckpt_path = info['path']\n",
    "    print(f'\\nEvaluating {name}...')\n",
    "    \n",
    "    model, config = load_vae_model(ckpt_path)\n",
    "    \n",
    "    privacy = evaluate_privacy(\n",
    "        model, attacker, adata, forget_idx, matched_neg_idx, retain_idx\n",
    "    )\n",
    "    utility = evaluate_utility(\n",
    "        model, X_holdout, labels_holdout, marker_idx, gene_names\n",
    "    )\n",
    "    \n",
    "    eval_results[name] = {\n",
    "        'privacy': privacy,\n",
    "        'utility': utility,\n",
    "        'training_time': info['time'],\n",
    "    }\n",
    "    \n",
    "    print(f'  AUC={privacy[\"mlp_auc\"]:.3f}, '\n",
    "          f'Advantage={privacy[\"mlp_advantage\"]:.3f}, '\n",
    "          f'ELBO={utility[\"elbo\"]:.1f}, '\n",
    "          f'Marker r={utility[\"marker_r\"]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCRUB Results (alpha_forget=1.0, 3 seeds):\n",
      "  AUC: 0.737 +/- 0.002\n",
      "  Advantage: 0.474 +/- 0.003\n",
      "\n",
      "Alpha_forget sweep (seed=42):\n",
      "alpha_f         AUC  Advantage   Marker r     ELBO\n",
      "--------------------------------------------------\n",
      "0.1           0.731      0.461      0.832    363.8\n",
      "1.0           0.736      0.473      0.832    363.8\n",
      "10.0          0.732      0.463      0.832    363.8\n",
      "\n",
      "Reference: Retrain AUC=0.523, Advantage=0.046\n",
      "Reference: Baseline AUC=0.783, Advantage=0.565\n",
      "\n",
      "SCRUB is the strongest modern baseline. If it fails on structured\n",
      "forget sets, this reinforces that the problem is fundamental to\n",
      "biological subpopulation unlearning, not a limitation of older methods.\n"
     ]
    }
   ],
   "source": [
    "# Aggregate af=1.0 seeds\n",
    "af1_aucs = []\n",
    "af1_advantages = []\n",
    "for seed in SEEDS:\n",
    "    key = f'af1.0_seed{seed}'\n",
    "    if key in eval_results:\n",
    "        af1_aucs.append(eval_results[key]['privacy']['mlp_auc'])\n",
    "        af1_advantages.append(eval_results[key]['privacy']['mlp_advantage'])\n",
    "\n",
    "print('SCRUB Results (alpha_forget=1.0, 3 seeds):')\n",
    "print(f'  AUC: {np.mean(af1_aucs):.3f} +/- {np.std(af1_aucs):.3f}')\n",
    "print(f'  Advantage: {np.mean(af1_advantages):.3f} +/- {np.std(af1_advantages):.3f}')\n",
    "print()\n",
    "\n",
    "print('Alpha_forget sweep (seed=42):')\n",
    "print(f'{\"alpha_f\":<10} {\"AUC\":>8} {\"Advantage\":>10} {\"Marker r\":>10} {\"ELBO\":>8}')\n",
    "print('-' * 50)\n",
    "for af in [0.1, 1.0, 10.0]:\n",
    "    key = f'af{af}_seed42'\n",
    "    if key in eval_results:\n",
    "        r = eval_results[key]\n",
    "        print(f'{af:<10.1f} {r[\"privacy\"][\"mlp_auc\"]:>8.3f} '\n",
    "              f'{r[\"privacy\"][\"mlp_advantage\"]:>10.3f} '\n",
    "              f'{r[\"utility\"][\"marker_r\"]:>10.3f} '\n",
    "              f'{r[\"utility\"][\"elbo\"]:>8.1f}')\n",
    "\n",
    "print()\n",
    "print('Reference: Retrain AUC=0.523, Advantage=0.046')\n",
    "print('Reference: Baseline AUC=0.783, Advantage=0.565')\n",
    "print()\n",
    "print('SCRUB is the strongest modern baseline. If it fails on structured')\n",
    "print('forget sets, this reinforces that the problem is fundamental to')\n",
    "print('biological subpopulation unlearning, not a limitation of older methods.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../outputs/p2/scrub/scrub_results.json\n"
     ]
    }
   ],
   "source": [
    "output = {\n",
    "    'method': 'scrub',\n",
    "    'dataset': 'PBMC',\n",
    "    'forget_type': 'structured',\n",
    "    'seeds': SEEDS,\n",
    "    'alpha_forget_sweep': [0.1, 1.0, 10.0],\n",
    "    'results': eval_results,\n",
    "    'summary': {\n",
    "        'af1.0': {\n",
    "            'mean_auc': float(np.mean(af1_aucs)),\n",
    "            'std_auc': float(np.std(af1_aucs)),\n",
    "            'mean_advantage': float(np.mean(af1_advantages)),\n",
    "            'std_advantage': float(np.std(af1_advantages)),\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(OUTPUT_BASE / 'scrub_results.json', 'w') as f:\n",
    "    json.dump(output, f, indent=2, default=str)\n",
    "\n",
    "print(f'Saved to {OUTPUT_BASE / \"scrub_results.json\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0n3bx73zfr1",
   "source": "## 4. Analysis\n\nSCRUB is the current state-of-the-art for machine unlearning (Kurmanji et al., NeurIPS 2023), so these results matter. At alpha_forget=1.0 across three seeds, MIA AUC is 0.737 +/- 0.002 with advantage 0.474. The baseline is 0.565. SCRUB barely moved it.\n\nThe alpha_forget sweep is the most telling part. A 100x range in the forget weight (0.1 to 10.0) changes the advantage by 0.018. The optimization is dominated by the retain objective no matter how aggressively the forget term is weighted. The alternating structure probably contributes: the retain steps just overwrite whatever the forget steps did.\n\nThe teacher-student distillation is doing almost nothing here. The forget divergence term tries to push the student away from the teacher on forget samples, but 30 cells cannot generate enough signal to overcome 28,000 retain samples pulling the model back. A larger forget set would give the forget objective more gradient mass, and SCRUB might perform differently at n=500 or n=1000. But the size ablation from extra-gradient (NB20) showed that larger structured forget sets also have stronger baseline memorization, so the problem scales on both sides. Utility is untouched (marker r=0.831-0.832, ELBO=363.8), which confirms the model is not changing in any meaningful way.\n\nIf the NeurIPS 2023 state-of-the-art fails this completely on a 30-cell structured forget set, the problem is not about finding the right method or the right hyperparameters. The VAE has distributed subpopulation information so thoroughly that local parameter adjustments cannot selectively remove it. This holds whether the adjustment is Fisher-weighted (SSD), latent-space (contrastive), or distillation-based (SCRUB). The only method that worked was DP-SGD, which never saw the forget set in the first place.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat4243",
   "language": "python",
   "name": "stat4243"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}